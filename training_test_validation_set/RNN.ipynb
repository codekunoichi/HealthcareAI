{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks in Python\n",
    "\n",
    "**Jessica Cervi**\n",
    "\n",
    "## Activity Overview \n",
    "\n",
    "A recurrent neural network (RNN) structure enables learning of sequence-to-sequence mappings. For example, in speech recognition one might have an input sequence of sounds or phonemes as input and the AI model learns to output a sequence of words or sentences. \n",
    "\n",
    "In this activity, we'll first look at how to assemble a simplified version of an RNN using the familiar MNIST digits dataset. Finally, we will train our networks and measure its accuracy to see if it makes a correct prediction.\n",
    "\n",
    "\n",
    "This activity is designed to help you apply the machine learning algorithms you have learned using the packages in `Python`. `Python` concepts, instructions, and starter code are embedded within this Jupyter Notebook to help guide you as you progress through the activity. Remember to run the code of each code cell prior to submitting the assignment. Upon completing the activity, we encourage you to compare your work against the solution file to perform a self-assessment.\n",
    "\n",
    "## Index:\n",
    "\n",
    "#### Week 3:  Recurrent Neural Networks\n",
    "\n",
    "- [Part 1](#part1) - Recurrent Neural Networks\n",
    "- [Part 2](#part2) - Setting up the Problem\n",
    "- [Part 3](#part3) - Creating the Model\n",
    "- [Part 4](#part4) - Compiling the Model \n",
    "- [Part 5](#part5) - Training the Model \n",
    "- [Part 6](#part6) - RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "## Recurrent Neural Networks <a></a>\n",
    "\n",
    "Similarly to what we did for DNNs, it’s helpful to understand at least some of the basics before getting to the implementation. At a high level, a recurrent neural network (RNN) processes sequences - whether daily stock prices, sentences, or sensor measurements - one element at a time while retaining a memory (called a state) of what has come previously in the sequence.\n",
    "\n",
    "\n",
    "Recurrent means that the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\n",
    "\n",
    "\n",
    "<img src=\"images/recurrent.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "In very simple words, with an RNN, your input data is passed into a cell, which we, along with outputting the activiation function's output, take that output and include it as an input back into this cell.\n",
    "This can work, but this means we have a new set of problems: How should we weigh incoming new data? How should we handle the recurring data? In other words, if we're not careful, that initial signal could dominate the entire model.\n",
    "Approaches both in training and in the RNN architecture itself have developed to combat this. \n",
    "\n",
    "### Long Short-Term Memory Structure\n",
    "\n",
    "The  long short-term memory structure (LSTM) is an RNN with a neural network structure. It adds learned “gating” functions to tell the network what portions of the past state or inputs or outputs are most relevant to achieve the learning goal. This enables the LSTM to learn the dynamics at multiple time scales – e.g., perhaps there are monthly periodic effects, in addition to more recent daily effects, that are important in predicting an output.\n",
    "\n",
    "The idea here is that we can have some sort of functions to determine what to forget from the previous cells, what to add from the new input data, what to output to new cells, and what to actually pass on to the next layer.\n",
    "\n",
    "<img src=\"images/lstm.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "## Setting up the Problem <a></a>\n",
    "\n",
    "Now let's work on applying an RNN to something simple, then we'll use an RNN on a more realistic use-case. We're going to use an RNN to predict our [MNIST](https://keras.io/api/datasets/mnist/) dataset, since that's a simple dataset, already in sequences, and we can understand what the model wants from us relatively easily.\n",
    "\n",
    "Run the code cell below for the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to import our data.\n",
    "\n",
    "Fill the ellipsis with `x_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist is a dataset of 28x28 images of handwritten \n",
    "# digits and their labels\n",
    "mnist = tf.keras.datasets.mnist  \n",
    "\n",
    "(..., y_train),(x_test,...) = mnist.load_data() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s take a look at one of the images in our dataset to see what we're working with. We will plot the first image in our dataset using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot the first image in the dataset\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Same as before, we can see the shape again of the dataset and individual samples.\n",
    "\n",
    "Run the code cell below to normalize the pixel values to the range from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "## Creating the Model <a></a>\n",
    "\n",
    "Now we are ready to build our model.\n",
    "\n",
    "The model type that we will be using is Sequential. The `Keras` class [`sequential`](https://keras.io/api/models/sequential/) is the easiest way to build a model in Keras. It allows you to build a model layer by layer.\n",
    "\n",
    "Run the code cell below to load the imports for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should all be straight-forward, similar to what we've seen previously. I this case, rather than  Conv layers, we're also using [`LSTM`](https://keras.io/api/layers/recurrent_layers/lstm/) as a layer type. \n",
    "\n",
    "The other new thing is `return_sequences` in the model structure below.\n",
    "This flag is used for when you're continuing on to another recurrent layer. If you are, then you want to return sequences. If you're **not going** to another recurrent-type of layer, then you don't set this to true. In our model below, our first LSTM layer is sequence to sequence mapping; the later layer than do additional feature extraction and classification, like in our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True))\n",
    "#model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "## Compiling the Model <a></a>\n",
    "\n",
    "Next, we need to compile our model. Compiling the model takes three parameters: optimizer, loss, and metrics.\n",
    "\n",
    "As before, the optimizer controls the learning rate. We will be using `adam` as our optmizer, as it is generally a good optimizer to use for many cases. The adam optimizer adjusts the learning rate throughout training. The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but as we saw, the time it takes to compute the weights will be longer.\n",
    "\n",
    "We will use `categorical_crossentropy` for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
    "\n",
    "To make things even easier to interpret, we will use the `accuracy` metric to see the accuracy score on the validation set when we train the model.\n",
    "\n",
    "In the code cell, fill in the ellipsis to set the argument `loss` equal to `'sparse_categorical_crossentropy'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "## Training the Model <a></a>\n",
    "\n",
    "\n",
    "Now, we will train our model. We will train the data in a similar way as we did for autoencoders.\n",
    "\n",
    "For our validation data, we will use the test set provided to us in our dataset, which we have split into `X_test` and `y_test`.\n",
    "The number of epochs is the number of times the model will cycle through the data. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch. For efficiency,  set the number of epochs to `3` in our model.\n",
    "\n",
    "Run the code cell below. **Note: this cell may take a few minutes to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=..., validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "What is the accuracy of our model after three epochs? Round your answer to two decimal digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICK ON THIS CELL TO TYPE YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "If you want to see the actual predictions that our model has made for the test data, we can use the [`predict` ](https://www.tensorflow.org/api_docs/python/tf/keras/Model) function. \n",
    "\n",
    "The predict function will give an array with 10 numbers. Again, as we saw in the DNN activity, these numbers are the probabilities that the input image represents each digit (0–9). The array index with the highest number represents the model prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict images in the test set, and show first four\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "print(y_pred[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "What is the predicted output of the second image shown above?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s compare this with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predicted results:\", y_pred_class[:4])\n",
    "print(\"actual results:   \", y_test[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that treating the mnist images as a \"sequence\" with an RNN is also able to extract relationships, and do a reasonably good job for subsequent classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "## RNN Language Model<a></a>\n",
    "\n",
    "RNNs are more typically used to map sequences to sequences (e.g., translate a sentence in one language to another language), or to map a sequence to a prediction for the next item in the sequence. In the example below, we'll use the same RNN structure as above, but this time train it on sequences of words from a corpus of texts.\n",
    "\n",
    "This example is optional -- but you might find it interesting as a final example for this week's content!\n",
    "\n",
    "Reference: the code below is inspired by the example at https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "books = ['alice_in_wonderland.txt', 'pride.txt', 'tale.txt', 'hamlet.txt', 'macbeth.txt']\n",
    "for book in ['alice_in_wonderland.txt']:\n",
    "    with open('data/'+book, 'r') as f:\n",
    "        texts.append(text_to_word_sequence(f.read()))\n",
    "\n",
    "# Tokenizer encodes word strings into integer identifiers for each unique word\n",
    "tokenizer = Tokenizer(num_words=1000) #limit to this number of most frequent words\n",
    "tokenizer.fit_on_texts(texts)\n",
    "corpus = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build data from `SEQLEN` consecutive words, followed by the next word as the \"label\" that we want to learn to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "\n",
    "input_words = []\n",
    "label_words = []\n",
    "for words in corpus:\n",
    "    for i in range(0, len(words) - SEQLEN, STEP):\n",
    "        input_words.append(words[i:i + SEQLEN])\n",
    "        label_words.append(words[i + SEQLEN])\n",
    "\n",
    "# Looking at our integer encoded data...\n",
    "for i in range(5):\n",
    "    print(input_words[i], label_words[i])\n",
    "num_words = max(label_words) + 1\n",
    "print(\"number of unique words:\", num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding word inputs and labels\n",
    "X = np.zeros((len(input_words), SEQLEN, num_words), dtype=np.bool)\n",
    "y = np.zeros((len(input_words), num_words), dtype=np.bool)\n",
    "for i, start_words in enumerate(input_words):\n",
    "    for j, w in enumerate(start_words):\n",
    "        X[i, j, w] = 1\n",
    "    y[i, label_words[i]] = 1\n",
    "print(\"X:\", X.shape, \"and y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our deep RNN model structure:\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(SEQLEN, num_words), activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below. **Note: this cell may take several minutes to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For now, we training with a small number of epochs, as an example. However, RNNs\n",
    "# are known for needing a LOT of training data and training time. So if you have the time, \n",
    "#you can increase this (e.g., to 100 or more epochs) to get much better results!\n",
    "model.fit(X, y, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('corpus_rnn_model') \n",
    "#model = keras.models.load_model('corpus_rnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input_words)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an starting list of start_words (which must be of\n",
    "# length SEQLEN and encoded with integer tokens), \n",
    "# generate and print an additional num_pred words after that.\n",
    "def generate_words(start_words, num_pred=10):\n",
    "    print(tokenizer.sequences_to_texts([start_words])[0],end=' | ')\n",
    "    for p in range(num_pred):\n",
    "        Xtest = np.zeros((1, SEQLEN, num_words))\n",
    "        for i, w in enumerate(start_words):\n",
    "            Xtest[0, i, w] = 1\n",
    "        ypred = model.predict(Xtest, verbose=0)[0].argmax()\n",
    "        print(tokenizer.sequences_to_texts([[ypred]])[0],end=' ')\n",
    "        start_words = start_words[1:] + [ypred]\n",
    "    print(\"\\n----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some examples, with start words taken from our text    \n",
    "for example in range(10):\n",
    "    start_idx = np.random.randint(len(input_words))\n",
    "    start_words = input_words[start_idx]\n",
    "    generate_words(start_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now some starter sentences of our own. Note that\n",
    "# these must only consist of words that were in our corpus,\n",
    "# so that the sentence is of the right length.\n",
    "start_text = \"alice ran from the queen with a mushroom soup pot\"\n",
    "start_text = \"funny red hare over there said the faster weeping turtle\"\n",
    "start_text = \"funny red hare over there said a faster weeping turtle\"\n",
    "start_text = \"funny red hare over here said a faster weeping turtle\"\n",
    "\n",
    "start_words = tokenizer.texts_to_sequences([start_text])[0]\n",
    "if len(start_words) != SEQLEN:\n",
    "    print(tokenizer.sequences_to_texts([start_words])[0], \"=>\", len(start_words), \"but need\", SEQLEN)\n",
    "else:\n",
    "    generate_words(start_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see the vocabulary of allowed words...\n",
    "all_words = tokenizer.sequences_to_texts([list(range(num_words))])\n",
    "#print(all_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
