{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference in Python\n",
    "\n",
    "**Author: Jessica Cervi**\n",
    "\n",
    "\n",
    "## Activity Overview\n",
    "\n",
    "This activity is designed to consolidate your knowledge about Bayesian methods and to teach you how to use them in `Python` using the `sklearn` library.\n",
    "\n",
    "We'll begin by seeing a simple, from scratch, implementation of the algorithm, and we will conclude by looking at how to run the same model in `sklearn`.\n",
    "\n",
    "\n",
    "This activity is designed to help you apply the machine learning algorithms you have learned using packages in `Python`. `Python` concepts, instructions, and starter code are embedded within this Jupyter Notebook to help guide you as you progress through the activity. Remember to run the code of each code cell prior to submitting the assignment. Upon completing the activity, we encourage you to compare your work against the solution file to perform a self-assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability Model and Classification\n",
    "\n",
    "In machine learning, we're often interested in a predictive modeling problem where we want to predict a class label for a given observation. For example, we might want to classify the species of plants based on the measurements of the flowers.\n",
    "\n",
    "Problems of this type are referred to as classification predictive modeling problems. The observation or input to the model is referred to as `X`, and the class label or output of the model is referred to as `y`.\n",
    "\n",
    "Together, `X` and `y` represent observations collected from the domain, i.e., a table or matrix (columns and rows or features and samples) of training data used to fit a model. The model must learn how to map specific examples to class labels or $y = f(X)$, which minimizes the misclassification.\n",
    "\n",
    "\n",
    "One approach to solving this problem is to develop a **probabilistic model**. From a probabilistic perspective, we are interested in estimating the conditional probability of the class label, given the observation.\n",
    "\n",
    "For example, a classification problem may have $k$ class labels $y_1, y_2, \\dots, y_k$ and $n$ input variables, $X_1, X_2, \\dots, X_n$. We can calculate the conditional probability for a class label with a given instance or set of input values for each column $X_1, X_2, \\dots, X_n$ as:\n",
    "\n",
    "$$P(y_i | X_1, X_2, \\dots, X_n).$$\n",
    "\n",
    "\n",
    "Bayes' theorem provides a principled way of calculating this conditional probability.\n",
    "\n",
    "A simple form of the calculation for Bayes' theorem is as follows:\n",
    "\n",
    "$$P(A|B) = P(B|A) * P(A) / P(B),$$\n",
    "\n",
    "where the probability that we are interested in calculating, $P(A|B),$ is called the posterior probability, and the marginal probability of the event $P(A)$ is called the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example of Bayesian Probability\n",
    "\n",
    "In this section, we will demonstrate the calculation using a small example on a machine learning dataset.\n",
    "\n",
    "We can generate a small binary (2 class) classification problem using the [`make_blobs()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function from the `scikit-learn` library.\n",
    "\n",
    "\n",
    "In the code cell below, we've imported the function `make_blobs` from `sklearn` and used that to generate 50 points with two numerical input variables, with each data point each assigned one of two classes. Modify the code cell below to generate 100 samples.\n",
    "\n",
    "Notice that the `random_state` argument is set to 1, ensuring that the same random sample of observations is generated each time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of generating a small classification dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate classification dataset\n",
    "X, y = make_blobs(n_samples=50, centers=2, n_features=2, random_state=1)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we visualize the first five entries for `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the input and output elements of the first five examples are also printed, showing that, the two input variables are indeed numeric and the class labels are either 0 or 1 for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Data\n",
    "\n",
    "\n",
    "We will model the numerical input variables using a **Gaussian** (or \"normal\") probability distribution. \n",
    "\n",
    "<img src=\"gauss.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "This can be achieved by using the function `norm` from the `SciPy` library. `SciPy` is a free and open-source `Python` library used for scientific and technical computing. `SciPy` contains modules for optimization, linear algebra, integration, special functions, signal and image processing, and other tasks common in science and engineering.\n",
    "\n",
    "The normal (Gaussian) probabililty distribution can be constructed by specifying the parameters of the distribution, i.e., the mean and standard deviation; the probability density function can then be sampled for specific values using the `norm.pdf()` function.\n",
    "\n",
    "We can estimate the parameters of the distribution from the dataset using the `mean()` and `std()` `NumPy` functions.\n",
    "\n",
    "In the code cell below, we've defined a custom-built function `fit_distribution()`, which takes a sample of data for one variable and returns a data distribution.\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# fit a probability distribution to a univariate data sample\n",
    "def fit_distribution(data):\n",
    "    # estimate parameters\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    print(mu, sigma)\n",
    "    # fit distribution\n",
    "    dist = norm(mu, sigma)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are interested in the conditional probability of each input variable. This means we need one distribution for each of the input variables and one set of distributions for each of the class labels or four distributions in total.\n",
    "\n",
    "In the code cell below, we split the data into groups of samples for each of the class labels.\n",
    "\n",
    "We start by separating the values of `X` that have corresponding `y` equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data into 0 class\n",
    "Xy0 = X[y == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, define the variable `Xy1`, which contains those values of `X` for which `y` is equal to 1.\n",
    "\n",
    "**HINT:** Follow the example in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data into 1 class\n",
    "Xy1 = X[y == ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the shape of the two sets we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xy0.shape, Xy1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll also be useful to plot our data, with class labels 0 in red, and class labels 1 in black. We can see that each group looks approximately Gaussian, with good separation between the two groups. So there's hope in building a good prediction model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(Xy0[:,0], Xy0[:,1], c='red')  # class 0\n",
    "plt.scatter(Xy1[:,0], Xy1[:,1], c='blue') # class 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these groups to calculate the prior probabilities for a data sample belonging to each group.\n",
    "\n",
    "This will be 50% exactly, given that we've created the same number of examples in each of the two classes; nevertheless, we will calculate these priors for completeness. We start with the prior for `Xy0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we just did, in the code below calculate the prior for `Xy1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priory1 = len(...) / len(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(priory0, priory1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call the `fit_distribution()` function that we defined to prepare a probability distribution for each variable, for each class label. Run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PDFs for y==0\n",
    "distX1y0 = fit_distribution(Xy0[:, 0])\n",
    "distX2y0 = fit_distribution(Xy0[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, compute the PDFs parameters for `Xy1`.\n",
    "\n",
    "**HINT:** Follow the example code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PDFs for y==1\n",
    "distX1y1 = fit_distribution(...[:, ...])\n",
    "distX2y1 = fit_distribution(...[:, ...])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Prediction\n",
    "\n",
    "Next, we can use the prepared probabilistic model to make a prediction.\n",
    "\n",
    "The independent conditional probability for each class label can be calculated using the prior for the class (50%) and the conditional probability of the value for each variable.\n",
    "\n",
    "The `probability()` function defined below performs this calculation. The value returned is a relative score rather than a probability, as the quantity is not normalized to $P(B)$ in Bayes' formula that normalization *is* handled by the corresponding sklearn function, which we will explore later below.\n",
    "\n",
    "The function below takes four arguments:\n",
    "\n",
    "- `X`: The input data\n",
    "- `prior`: The prior distribution calculated above\n",
    "- `dist1/dist2`: The conditional distribution for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to calculate the probability for an example belonging to each class.\n",
    "\n",
    "First, we can select an example to be classified - in this case, the first example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify one example\n",
    "Xsample, ysample = X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below,  we can calculate the score of the example belonging to the first class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py0 = probability(Xsample, priory0, distX1y0, distX2y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the code above to compute the probabily for the second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py1 = probability(Xsample, priory1, distX1y1, distX2y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scores above, which will be the resulting classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn` implementation\n",
    "\n",
    "Of course, the `sklearn` library offers an implementation of the algorithm above.\n",
    "\n",
    "It provides three implementations, one for each of the three main probability distributions; for example, `BernoulliNB`, `MultinomialNB`, and `GaussianNB` for binomial, multinomial, and Gaussian-distributed input variables, respectively.\n",
    "\n",
    "To use a `sklearn`  Bayes' model, first, the model is defined, then it's fit on the training dataset. Once fit, probabilities can be predicted via the `predict_proba()` function, and class labels can be predicted directly via the `predict()` function.\n",
    "\n",
    "The complete example of fitting a Gaussian Bayes' model (`GaussianNB`) to the same test dataset is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of gaussian naive bayes\n",
    "\n",
    "#importing libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = GaussianNB()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# select a single sample\n",
    "Xsample, ysample = [X[0]], y[0]\n",
    "# make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted Probabilities: ', yhat_prob)\n",
    "# make a classification prediction\n",
    "yhat_class = model.predict(Xsample)\n",
    "print('Predicted Class: ', yhat_class)\n",
    "print('Truth: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits the model on the training dataset, then makes predictions for the same first example that we used in the prior example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Xsample:', Xsample)\n",
    "print('Predicted Probabilities: ', yhat_prob)\n",
    "# make a classification prediction\n",
    "yhat_class = model.predict(Xsample)\n",
    "print('Predicted Class: ', yhat_class)\n",
    "print('True Class: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help visualize this, we repeat the plot of our data but now adding the point whose class we are trying to predict, as a black 'X':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xy0[:,0], Xy0[:,1], c='red')  #class 0\n",
    "plt.scatter(Xy1[:,0], Xy1[:,1], c='blue') #class 1\n",
    "plt.scatter(Xsample[0][0], Xsample[0][1], c='black', marker='X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again, this time for a data point we haven't seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsample = np.array([[-9.0, -5.0]]) #new data point\n",
    "print('Xsample:', Xsample)\n",
    "\n",
    "# make a probabilistic prediction\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "\n",
    "print('Predicted Probabilities: ', yhat_prob)\n",
    "yhat_class = model.predict(Xsample)\n",
    "print('Predicted Class: ', yhat_class)\n",
    "\n",
    "plt.scatter(Xy0[:,0], Xy0[:,1], c='red')  #class 0\n",
    "plt.scatter(Xy1[:,0], Xy1[:,1], c='blue') #class 1\n",
    "plt.scatter(Xsample[0][0], Xsample[0][1], c='black', marker='X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
